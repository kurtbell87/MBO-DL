The Practitioner’s Compendium: Baked-In Domain Knowledge for Greenfield Machine Learning SystemsExecutive Summary: The Gap Between Theory and EngineeringThe discipline of machine learning (ML) is characterized by a distinct and often painful schism between its academic literature and its industrial application. Textbooks and conference papers typically focus on novel architectures, mathematical proofs of convergence, and achieving state-of-the-art (SOTA) results on static benchmark datasets like ImageNet or GLUE. However, the day-to-day reality of the ML practitioner—the engineer tasked with deploying a recommendation system, a robotic controller, or a fraud detection engine from scratch—is governed by a different set of rules. This "tribal knowledge" consists of heuristics, debugging rituals, and architectural "safe bets" that are rarely codified but form the bedrock of successful system building.This report compiles this baked-in domain knowledge, aggregating the "folklore" of the field into a structured guide for approaching greenfield projects. It moves beyond the what of algorithms to the how of engineering: how to verify a data pipeline before a model ever sees it, how to select a "boring" architecture that guarantees delivery, and how to navigate the treacherous waters of reinforcement learning implementation where the smallest code-level detail can silence a reward signal. Drawing from the collective experience of practitioners at organizations like Google, OpenAI, and the broader open-source community, this document serves as a comprehensive manual for the unwritten curriculum of machine learning engineering.Part I: The Philosophy of the Greenfield ProjectBefore a single tensor is instantiated or a GPU is provisioned, the experienced practitioner engages in a phase of defensive engineering. The consensus among industry veterans is that the primary risks in a greenfield ML project are not algorithmic, but structural. The "silent killers" are data leakage, infrastructure instability, and misaligned objectives. The initial phase of any project is defined not by the complexity of the model, but by the robustness of the pipeline that feeds it.1.1 The "No-ML" Baseline and the Rule of First LaunchA paradox lies at the heart of Google’s "Rules of Machine Learning": the first rule of ML is to not use ML. For a greenfield project, the introduction of a stochastic, non-deterministic component (the model) into a deterministic system (the software stack) introduces massive complexity. The "baked-in" wisdom dictates a strict, three-stage evolution for any new product feature, a methodology designed to isolate failure modes and establish credibility.Stage 1: The Heuristic PhaseIf the task is to predict fraud, the practitioner does not start with a Transformer. They start with a database query: "If transaction amount > $10,000, flag." If the task is recommending content, the starting point is "Sort by popularity" or "Sort by recency." This is not merely a placeholder; it is a critical baseline.Data Harvesting: A heuristic system allows the engineering team to begin collecting data immediately. The interactions users have with the heuristic (e.g., did they click the popular item?) become the labeled training data for the future model.Infrastructure Stress Test: Deploying a heuristic tests the serving infrastructure, logging pipelines, and latency constraints without the opacity of a neural network. If the pipeline drops logs or fails under load with a simple if-then statement, it will certainly fail with a matrix multiplication.The "Neutral" First Launch: Some teams advocate for a "neutral" launch where the ML system is deployed but explicitly de-prioritized to avoid distracting the team with optimization games before the pipes are clean. The goal is to establish a system where data flows correctly from the user to the log, to the training bucket, and back to the serving layer.Stage 2: The Linear BaselineOnce data infrastructure is robust, the next step is rarely a Deep Neural Network (DNN). It is a logistic regression or a simple decision tree.Decoupling Bugs: This approach decouples infrastructure bugs from modeling bugs. If a heuristic system fails, the error is in the code. If a neural network fails, the error could be in the code, the data, the hyperparameters, or the optimization landscape. A linear model provides a "sanity check" baseline. If a complex model cannot outperform a logistic regression, the problem may lack sufficient signal, or the complex model is misconfigured.Interpretability: Linear weights provide immediate insight into feature importance. If the model assigns a high positive weight to a feature that should be negative, it indicates a data quality issue (e.g., days_since_active being -1 for new users) rather than a subtle non-linear interaction.Stage 3: Complexity InjectionOnly when the linear baseline plateaus, and the infrastructure is proven stable, is complexity introduced. This follows the principle that most gains in practical ML come from great features, not great algorithms. The transition to deep learning is a high-cost maneuver that should only be undertaken when the "low-hanging fruit" of feature engineering and heuristics has been exhausted.1.2 The "Karpathy Recipe" for Neural Network TrainingWhen the decision is finally made to train a deep network, practitioners often cite Andrej Karpathy’s "Recipe for Training Neural Networks" as the definitive debugging methodology. The core tenet of this recipe is the rejection of complexity at the start. One does not simply "train a ResNet." One builds a biological connection with the data, iteratively validating hypotheses before scaling up.The "Overfit a Single Batch" RitualThis is perhaps the most universally cited sanity check in deep learning folklore. Before training on a dataset, the practitioner must attempt to train the model on a single batch of examples (e.g., 2 to 4 images or sentences) to achieve zero loss.The Procedure:Sample: Extract a tiny subset of data $(x, y)$, typically 2-4 examples.Strip Regularization: Disable all regularization mechanisms. Set Dropout to 0.0. Set Weight Decay to 0.0. Turn off data augmentation.Train: Run the optimizer for hundreds of epochs on this fixed batch.Expectation: The training accuracy should hit 100%, and the loss should approach 0.0000 (machine epsilon).The Diagnosis:Failure to Overfit: If the model cannot overfit a single batch, the architecture is fundamentally incapable of memorizing the input (under-parameterized), or there is a catastrophic bug in the data loader (e.g., labels are shuffled, inputs are zeroed out, gradients are detached).Plateau: If the loss goes down but plateaus above zero, the learning rate might be too low, or the initialization is poor.Explosion: If the loss explodes (NaNs), the learning rate is too high, or gradients are unclipped.This test is the "Hello World" of model development. Passing it proves that the model has the capacity to learn and that the gradient flow is mechanically functional. It verifies that the labels are reachable from the inputs given the current architecture.Visualizing the "Pre-Model" TensorA common silent failure mode in greenfield projects is "blind data loading." A practitioner might write a complex augmentation pipeline (random crops, horizontal flips, color jitters) and feed the result into the model. If the normalization step accidentally divides by zero, or the cropping logic extracts black pixels, the model will fail silently or learn to predict the bias.The Fix: Visualize the exact tensor entering the network immediately before the forward pass. This requires writing a "decoder" that takes the normalized float tensors (e.g., mean 0, variance 1) and converts them back into viewable images or readable text.What to look for:Normalization Artifacts: Are the images all gray? Are the values NaN?Augmentation Errors: Is the bounding box still over the object after the image was cropped? Is the text sequence reversed?Label Alignment: Is the label "Dog" actually attached to the picture of a dog?Silent Padding: Are the inputs mostly padding tokens (zeros) because of poor batching strategies?This "source of truth" check saves weeks of debugging time by identifying problems in data preprocessing and augmentation before the expensive training loop begins.1.3 Random Seeds and DeterminismWhile stochasticity is inherent to training (Stochastic Gradient Descent), it is the enemy of debugging. Tribal knowledge dictates that one must "fix the seed" (random, numpy, torch/tensorflow) at the very beginning of the project.The Nuance of Determinism:In modern frameworks like PyTorch (specifically with cuDNN backends), absolute determinism can come at a performance cost because it requires disabling non-deterministic algorithm benchmarking. However, during the "bringing up" phase of a greenfield project, reproducibility is worth the speed penalty. If a bug appears in epoch 4, it must appear in epoch 4 every time.Implementation: It is not enough to seed the random number generator. One must also seed the dataloader workers (which often fork processes with new random states) and force the GPU backend to use deterministic algorithms.The "Flaky Test" of ML: If a model's performance varies wildly between runs with different seeds, the architecture is likely unstable, or the dataset is too small. However, during debugging, removing this variance allows the engineer to attribute changes in loss to changes in code, not changes in random initialization.Part II: Feature Engineering FolkloreWhile deep learning has promised to automate feature engineering, this promise holds primarily for perceptual tasks (vision, audio). For tabular data—which constitutes the majority of business use cases (fraud, churn, click-through rate)—feature engineering remains the primary lever for performance. The "tribal knowledge" here focuses on how to represent data so that models can learn efficiently.2.1 The Tabular Data Consensus: GBDT vs. Deep LearningIn the greenfield setting for tabular data (user logs, financial transactions, sensor readings), a fierce debate exists between Gradient Boosted Decision Trees (GBDTs) and Deep Learning.The Expert Consensus:Default to GBDTs: Algorithms like XGBoost, LightGBM, and CatBoost are the industry "workhorses". They are robust to unscaled data, handle missing values natively, and provide interpretability (feature importance). In a greenfield project, where the data distribution is often messy and "dirty" (non-smooth manifolds, discrete jumps, mixed types), GBDTs offer the fastest path to a high-performance baseline.The Deep Learning Caveat: Deep Learning (e.g., TabNet, ResNet-for-tabular) can match or exceed GBDTs, but typically only with significantly more tuning and data preprocessing. Deep networks prefer smooth, continuous manifolds and struggle with the sharp decision boundaries often found in tabular business logic.The Hybrid Approach: A common pattern in mature systems is to use GBDTs for the core logic and Neural Networks to generate embeddings from unstructured text or image data associated with the tabular records.Heuristic: If your data looks like an Excel spreadsheet, start with CatBoost. If it looks like a JPEG or a WAV file, start with a ResNet or Transformer.2.2 Handling Categorical VariablesHandling categorical data is rife with tribal knowledge that prevents "silent" model degradation.The Cardinality TrapLow Cardinality (e.g., < 10 categories): One-Hot Encoding is standard. It is simple, interpretable, and works well with linear models and trees.High Cardinality (e.g., User IDs, Zip Codes, Product SKUs): One-Hot encoding leads to sparse matrices that explode memory usage and make optimization difficult (the "curse of dimensionality").The "Target Encoding" Trap: Replacing a category with the average target value for that category is powerful but prone to leakage. If the model sees the target value embedded in the feature, it will overfit. The tribal trick is to use Leave-One-Out encoding or add Gaussian noise to the encoded values during training to prevent the model from memorizing the mapping.The Deep Learning Solution: Use Embeddings. Learned vector representations (like Word2Vec but for Zip Codes) allow the model to learn semantic similarities between categories. For example, the model might learn that Zip Code 90210 and 90024 are "close" in vector space because they share similar target behaviors, even if their numerical IDs are far apart.The "Rare Category" BucketA common mistake is letting a model see a category in training that appears only once or twice. The model will overfit to the target value of those specific examples, essentially "memorizing" them.The Fix: Apply a frequency threshold. Map all categories with fewer than $N$ occurrences (e.g., $N=5$ or $N=10$) to a generic <UNK> (Unknown) or Other token. This forces the model to learn a robust representation for rare events rather than memorizing noise. This also handles the "cold start" problem for new categories that appear during inference—they simply map to <UNK>.2.3 Numerical Hygiene and "The Kitchen Sink"Google’s Rule #2 dictates: "First, design and implement metrics." But Rule #4 warns: "Keep the first model simple.".Scaling and DistributionFor neural networks, input normalization is non-negotiable. Inputs should be roughly mean 0 and unit variance.The "StandardScaler" Trap: Do not blindly apply StandardScaler (subtract mean, divide by std). If the data has extreme outliers (e.g., income data where one user earns billions), StandardScaler will squash the "normal" range into a tiny interval, destroying the signal for the majority of users.The Fix: Use Log-transformation (log(x+1)) for power-law distributions before scaling. Alternatively, use Quantile Transforms (Rank Gauss) to force the input distribution into a Gaussian shape, which is the ideal input for neural networks.Missing Values: The Signal in the NoiseNever impute the mean blindly.Why: "Missingness" is often a signal itself (Informative Missingness). A user missing a "Credit Score" field is not the same as a user with an "Average" credit score; they might be a ghost or a fraudster.The Fix: If a value is missing, impute it with a sentinel value (e.g., -1 or the mean) AND add a binary indicator feature is_missing. This allows the model to separate the imputed value from the "true" values and learn a specific weight for the missing condition.Interaction FeaturesIn GBDT models, explicit interaction features (e.g., Feature A / Feature B) are less critical than in linear models because trees can learn interactions by splitting on A then B. However, for Deep Learning on tabular data, MLPs are notoriously bad at learning multiplicative relationships (the "multiplication problem").Heuristic: If domain knowledge suggests that the ratio of two numbers (e.g., Debt / Income) is important, engineer that feature explicitly. Do not rely on the neural network to "discover" division.Part III: Architecture Selection and Training DynamicsIn a greenfield project, "novelty" is a risk factor. The goal is to reach a baseline performance as fast as possible. The "baked-in" knowledge here is about choosing the path of least resistance.3.1 The "Boring Architecture" ManifestoPractitioners operate under a "risk budget." If the data pipeline is new (high risk), the architecture must be boring (low risk). "Boring" in this context means "proven," "robust," and "easy to debug."Computer Vision BackbonesDo not design a custom Convolutional Neural Network (CNN). The "unit of currency" in computer vision is the ResNet-50.Why ResNet? It is deep enough to learn complex features, shallow enough to train easily (thanks to skip connections), and pre-trained weights are available in every framework. It strikes the optimal balance between accuracy and computational cost for a baseline.Beyond ResNet: If efficiency is paramount (e.g., mobile deployment), EfficientNet or MobileNet are the standard choices.The "Don't Be a Hero" Rule: If a paper was published last month and claims SOTA, do not use it for a greenfield project. Use the model that has been SOTA for 3 years. The implementation details of new papers are often unstable or undocumented.NLP BaselinesFor Natural Language Processing, the landscape has shifted to Transformers.The Baseline: Use a DistilBERT or BERT-base for classification/embedding tasks. These models are distilled versions of larger models, offering 95% of the performance for half the compute.The "Tokenizer" Trap: The choice of tokenizer is as important as the model. Ensure the tokenizer matches the pre-trained model exactly. A mismatch (e.g., using a cased tokenizer with an uncased model) will result in silent degradation.The "Head" Surgery StrategyThe standard practice for Transfer Learning is to take a pre-trained backbone, freeze its weights, and train a new "Head" (Linear Layer) on the specific dataset.Why Freeze? If the backbone is not frozen, the large gradients from the randomly initialized head can "destroy" the delicate pre-trained feature detectors in the backbone during the first few epochs.The Procedure:Freeze backbone.Train Head until convergence.Unfreeze backbone (partially or fully).Fine-tune with a lower learning rate (e.g., 1/10th of the original rate).3.2 Hyperparameter "Safe Bets"While every problem is different, there are "basin of attraction" settings that serve as excellent starting points. These are the constants that engineers type into config files without thinking.HyperparameterThe "Safe Bet"ReasoningOptimizerAdamWAdamW fixes the weight decay implementation in Adam. It is generally more robust than SGD for greenfield projects because it handles learning rate scaling per-parameter.Learning Rate3e-4 (0.0003)The "Karpathy Constant." This value is legendary for being "good enough" for a surprisingly wide range of architectures (ResNet, GPT, etc.).Batch Size32 or 64Large batch sizes (thousands) often degrade generalization (the "generalization gap") unless the learning rate is scaled carefully. 32 is a safe default for stability.Weight Decay1e-4 or 1e-5Standard regularization. If using AdamW, ensure this is decoupled from the gradient update.InitializationKaiming (He)For ReLU networks. Using Xavier (Glorot) init on ReLU networks can lead to vanishing gradients because Xavier assumes linear/sigmoid activations.3.3 Silent Bugs in Deep Learning"Silent bugs" are the most dangerous category of errors in ML. Unlike compilation errors, silent bugs allow the code to run and the loss to decrease, but the result is garbage.Broadcasting ErrorsIn Python/Numpy/PyTorch, operations between arrays of different shapes can be "broadcast" to match.The Bug: loss = prediction - target. If prediction is shape (64, 1) and target is (64,), standard libraries will broadcast this to (64, 64), calculating the difference between every prediction and every target. The result is a matrix of 4096 errors instead of a vector of 64 errors. The code runs, the loss looks somewhat reasonable, but the gradient is mathematically nonsensical.The Fix: Assert shapes explicitly: assert prediction.shape == target.shape. Use view() or reshape() to ensure dimensions match before arithmetic.The "Flip" BugIn image data, the convention for dimensions varies. OpenCV uses (Height, Width, Channels), PyTorch uses (Channels, Height, Width).The Bug: Passing a (H, W, C) image to a model expecting (C, H, W). If $C=3$ and $H$ or $W$ is close to 3 (e.g., small patches), the code might run but interpret the color channels as spatial dimensions.The Fix: Explicitly permute dimensions using image.permute(2, 0, 1) and verify shapes.Loader NondeterminismIf data loaders rely on random.random() without a worker-seeded RNG, each worker process might inherit the same random state from the parent process.The Result: Each worker produces the exact same sequence of "random" augmentations. The effective diversity of the dataset is divided by the number of workers.The Fix: Use the worker_init_fn in PyTorch dataloaders to seed the RNG based on the worker ID.Part IV: The Black Art of Reinforcement Learning (RL)If supervised learning is "hard," Reinforcement Learning is "diabolical." In RL, the supervision signal (reward) is sparse, delayed, and noisy. If the code has a bug, the agent simply learns to do nothing, or spins in circles, and the practitioner has no idea why. The "baked-in" knowledge in RL is heavily focused on implementation details that stabilize this chaotic process.4.1 The "Implementation Matters" RealityResearch has shown that the difference between "state-of-the-art" RL algorithms (like PPO vs. TRPO) is often less significant than the code-level optimizations used in the implementation. A "stock" implementation of Proximal Policy Optimization (PPO) from a textbook description will likely fail to solve even simple environments. It is the "37 Implementation Details" found in libraries like CleanRL and OpenAI Baselines that make it work.4.2 The "Good RL" ChecklistTo do "Good RL," one must adhere to a strict set of preprocessing and architectural rituals. These are not optional; they are prerequisites for convergence.Observation NormalizationThis is arguably the single most critical "trick" in continuous control RL.The Problem: Neural networks expect inputs with unit variance. In RL, state values can vary wildly (e.g., joint velocities vs. positions). A position might range from $[-1, 1]$ while a velocity ranges from $[-100, 100]$. The gradients for the velocity inputs will dominate the updates.The Mechanism: Maintain a running mean $\mu$ and standard deviation $\sigma$ of the observations encountered so far. Normalize the input state $s$ to $\frac{s - \mu}{\sigma + \epsilon}$ before passing it to the network.Tribal Tip: Do not update the running mean/variance during testing/inference! Freeze the statistics to ensure consistent evaluation.Reward Scaling and ClippingThe scale of the reward determines the scale of the gradients.Reward Clipping: In environments like Atari, rewards are often clipped to the range $[-1, 1]$. This prevents a "lucky" trajectory with a massive reward from destabilizing the policy weights.Reward Scaling: Alternatively, divide rewards by the standard deviation of the rolling discounted return. This ensures the Value function targets are within a learnable range (e.g., roughly unit variance). If the Value function tries to predict a return of 10,000, the gradients will be huge, leading to "catastrophic forgetting" of the policy.Orthogonal InitializationFor PPO and policy gradient methods, initializing weights with Orthogonal Initialization (and scaling the output layers of the policy to have very small weights, e.g., 0.01) significantly improves convergence.The Logic: Orthogonal initialization preserves the norm of the input vector as it passes through the layers, preventing vanishing/exploding gradients at initialization. Scaling the policy output weights to be small ensures the agent starts with a nearly uniform (random) policy, maximizing exploration early in training.Vectorized EnvironmentsThe "Vectorized Architecture" is a standard pattern for efficiency and stability.The Technique: Run $N$ environments in parallel (e.g., 8 or 16). Collect a batch of experience from all of them simultaneously.Why it Matters: This breaks the temporal correlation of the data. In a single environment, state $s_{t+1}$ is highly correlated with $s_t$. SGD assumes independent and identically distributed (i.i.d.) data. By gathering data from 16 parallel "universes," the batch becomes more diverse and i.i.d., stabilizing the gradient estimate.4.3 PPO Specific "Folklore"Proximal Policy Optimization (PPO) is the default algorithm for greenfield RL due to its balance of simplicity and performance. However, the standard implementation details are mandatory:Generalized Advantage Estimation (GAE): GAE is a technique to balance bias and variance in the advantage calculation. It introduces a parameter $\lambda$. Using GAE is standard practice; calculating raw Advantage ($R_t - V(s_t)$) is rarely sufficient for complex tasks.Value Function Clipping: Just as PPO clips the policy update to stay within a "trust region," effective implementations also clip the Value function update. This prevents the Value network from overfitting to a specific batch’s returns, which can cause the Critic to lose its ability to generalize.Global Gradient Clipping: Clip the norm of the gradients (e.g., to 0.5) before the optimizer step. This prevents "exploding gradients" from destroying the policy during the early, volatile phases of training.4.4 Debugging the RL AgentTribal knowledge suggests a hierarchy of debugging environments to isolate failure modes:The "Sanity" Env: Create a trivial environment where the optimal action is obvious (e.g., "Always Go Right"). If the agent fails to solve this in minutes, the code is broken. Do not proceed to complex tasks.CartPole/Pendulum: The standard "Hello World" of RL. If PPO cannot solve CartPole in <50k steps, the implementation is buggy or the hyperparameters are wildly off.The Entropy Monitor: The Entropy curve is the heartbeat of the agent.Entropy High/Flat: The agent is purely random. It is not learning from the reward signal.Entropy Crashes to Zero: The agent has prematurely converged to a deterministic (likely suboptimal) policy.Ideal Behavior: Entropy should start high and slowly decrease as the agent becomes more confident in its strategy.Visualizing the Value Function: Plot the Critic's predictions $V(s)$ against the true returns $G_t$. If $V(s)$ is flat or uncorrelated with $G_t$, the Critic is failing. If the Critic fails, the Actor (which relies on the Critic's Advantage estimate) is learning from random noise.Part V: Production Readiness and "The Test Score"A model that achieves high accuracy in a Jupyter notebook is not a production system. It is a prototype. The transition to production introduces the concept of Technical Debt in ML systems—debt that is "hidden" because it exists not in code complexity, but in system entanglements and data dependencies. Google’s "ML Test Score" paper provides a rubric for assessing the maturity of an ML pipeline.5.1 Training-Serving SkewThis is the most common cause of production failure. Skew occurs when the code or data used at inference time differs from training time.The "Exact Score" Test: This is the gold standard for skew detection.Take a single example or a batch of examples.Compute the model’s prediction in the training pipeline (Python, offline).Compute the prediction for the same example in the serving infrastructure (often C++, Java, or a different Python environment).The Requirement: The scores must be bit-exact identical. If they differ even by floating-point noise ($10^{-6}$), there is a discrepancy. Today's rounding error is tomorrow's catastrophic failure when the input distribution shifts.Common Causes:Different library versions (Pickle vs. ONNX).Different implementation of feature logic (e.g., "Wait 24 hours" logic implemented differently in SQL vs. Java).Floating point precision (Float32 vs. Float64).5.2 The 28-Test Rubric (Selected Highlights)The "ML Test Score" rubric consists of 28 tests. Seasoned engineers insist on passing the "Critical Few" before deployment.Data TestsFeature Expectations Schema: Intuitions about data must be codified. "Age is always positive." "Embedding vector norm is 1.0." These expectations should be tested on input data during both training and serving.Pipeline Privacy Controls: Can we delete a user's data from the training set and the model? With regulations like GDPR, "Machine Unlearning" or efficient retraining is a requirement, not a feature.Model TestsOffline Proxy Correlation: Does offline accuracy actually correlate with online business metrics? If LogLoss goes down but Revenue doesn't go up, the team is optimizing the wrong objective. This requires running A/B tests with "degraded" models to verify the correlation.Staleness Sensitivity: If the pipeline breaks and the system serves a 3-day old model, how much performance is lost? In dynamic environments (ads, news), a model that is 24 hours old might be useless. This decay rate must be known to set on-call severity levels.Infrastructure TestsReproducibility: Can you retrain the model to the same performance twice? If not, debugging is impossible because you cannot isolate the effect of a code change from the random variance of training.Canary Deployment: Always deploy the new model to a small fraction of traffic (e.g., 1%) and monitor error rates before full rollout.Monitoring TestsPrediction Drift: Monitor the statistical distribution of the model's outputs $P(Y)$. If the mean prediction shifts from 0.05 to 0.10, but the ground truth rate hasn't changed, the model is miscalibrated.Feature Drift: Monitor the distribution of inputs $P(X)$. If "Null" values suddenly spike in a feature, the upstream data source might be broken.5.3 Feedback Loops and System DynamicsIn production, models often affect the data they learn from.The "Echo Chamber" Loop: In recommendation systems, the model determines what data it generates. If the model recommends "Clickbait," users click it. The model learns "Clickbait is good." The system spirals into a local minimum of low-quality content.The Fix: Exploration. Reserve a small slice of traffic (epsilon-greedy) to show random or diverse content. This gathers unbiased data that reflects true user preferences, not just preferences within the model's current regime.Part VI: Conclusion and SynthesisThe transition from academic theory to industrial practice is a shift from optimizing algorithms to optimizing pipelines. The "baked-in" knowledge of the field suggests a hierarchy of needs for the greenfield project, a roadmap that prioritizing survival over sophistication.Infrastructure over Algorithms: Build a solid end-to-end pipeline with a heuristic baseline before training a model. If the pipes leak, the quality of the water doesn't matter.Sanity over Novelty: Use the "Karpathy Recipe" to verify the pipeline. Overfit a single batch. Visualize inputs. Fix seeds. Reject complexity until simplicity has failed.Simplicity over Complexity: Use GBDTs for tabular data; use ResNet/BERT for perceptual data. Use Adam with $3e-4$. These are the "safe bets" that allow the engineer to focus on the data.Details over Concepts (in RL): Code-level optimizations (normalization, clipping, orthogonal init) define success in Reinforcement Learning more than the choice of high-level algorithm. The "magic" is in the implementation details.Monitoring over Faith: Assume the model is broken until proven otherwise. Test for Training-Serving skew, monitor input distributions, and validate that offline metrics correlate with online reality.By adhering to these unwritten rules—the "folklore" of the trade—the practitioner insulates themselves from the most common failure modes. They turn the "black magic" of machine learning into a disciplined, reproducible engineering practice. The goal is not just to build a model that learns, but to build a system that works.
